# -*- coding: utf-8 -*-
"""HW3_810602161.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QptHMmEuUDA2SrUooqcg6Y1Z9QZjj1i7
"""

import pandas as pd
from google.colab import files

uploaded = files.upload()
milling = pd.read_csv('milling_machine.csv')

print("Sakhtare kolie dadeha:")
print(milling.info())

print("Kholase amarie dadeha:")
print(milling.describe())

import numpy as np

milling['Failure Binary'] = np.where(milling['Failure Types'].isnull(), np.nan,
                                (milling['Failure Types'] != 'No Failure').astype(int))


# Hazfe sutune failure types ghabl az binary shodan
milling = milling.drop(columns=['Failure Types'])

# Maghadire gomshode dar har sotoon
missing_count = milling.isnull().sum()
missing_ratio = (milling.isnull().mean() * 100).round(2)

missing_milling = pd.DataFrame({'Missing Count': missing_count,
                                'Missing Ratio (%)': missing_ratio})

print("Maghadire namojoud dar har sotoon:")
print(missing_milling)

import seaborn as sns
import matplotlib.pyplot as plt

# Sotoonhaye adadi
sutun_adadi = milling.select_dtypes(include=['float64', 'int64'])

corr_matrix = sutun_adadi.corr()
print(corr_matrix)

# Vabastegie vizhegiha
print("Hambastegie vizhegiha ba vaziate abzar (Binary shodeye sotoon failure types):")
print(corr_matrix['Failure Binary'].drop('Failure Binary').sort_values(ascending=False))

# Heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True, cbar_kws={"shrink": .8})
plt.title('Matrix hambastegie vizhegiha:', fontsize=14)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 3 vizhegi ba hambastegie bishtar
correlations = corr_matrix['Failure Binary'].drop('Failure Binary').abs().sort_values(ascending=False)
vizhegiha = correlations.head(3).index.tolist()
print("3 vizhegi ba bishtarin hambastegi ba khuruji:")
print(vizhegiha)

# Rasme nemudare faravani
for feature in vizhegiha:
    plt.figure(figsize=(8, 4))
    sns.histplot(milling[feature], bins=30, kde=False)
    plt.title(f'Nemudare faravanie vizhegie {feature}')
    plt.xlabel(feature)
    plt.ylabel('Tedad moshahedat')
    plt.grid(True)
    plt.show()

# Jaygozinie maghadire gomshode ba miangin baraye sutunhaye adadi
sutun_adadi_gomshode = missing_milling.index[missing_milling['Missing Count'] > 0]

for col in sutun_adadi_gomshode:
    if milling[col].dtype in ['float64', 'int64']:
        milling[col] = milling[col].fillna(milling[col].mean())

# Jaygozinie maghadire gomshode ba mode baraye sutunhaye daste bandi
sutun_daste_gomshode = missing_milling.index[missing_milling['Missing Count'] > 0]

for col in sutun_daste_gomshode:
    if milling[col].dtype == 'object':
        milling[col] = milling[col].fillna(milling[col].mode()[0])

# Barresi mojadad
missing_count_after = milling.isnull().sum()
missing_ratio_after = (milling.isnull().mean() * 100).round(2)

missing_milling_after = pd.DataFrame({
    'Missing Count': missing_count_after,
    'Missing Ratio (%)': missing_ratio_after
})

print("Maghadire namojoud pas az jaygozini:")
print(missing_milling_after)

from sklearn.preprocessing import StandardScaler

target_cols = ['Failure Binary', 'Failure Types']

sutun_adadi = milling.select_dtypes(include=['float64', 'int64']).columns
sutun_adadi = [col for col in sutun_adadi if col not in target_cols]

# Shey'e standard
scaler = StandardScaler()

# Standardsazi sutunhaye adadi
milling[sutun_adadi] = scaler.fit_transform(milling[sutun_adadi])

print("Dadeha pas az standardsazi:")
print(milling[sutun_adadi].describe())

# # Normal sazi
# from sklearn.preprocessing import MinMaxScaler

# # Shey'e Normal sazi
# scaler = MinMaxScaler()

# milling[sutun_adadi] = scaler.fit_transform(milling[sutun_adadi])

# print("Dadeha pas az normal sazi:")
# print(milling[sutun_adadi].describe())

# Sutune jadid ba barchasbe "no failure" va "failure"
milling['Failure Label'] = np.where(milling['Failure Binary'] == 0, 'No Failure', 'Failure')

# Tedad barchasbha
failure_counts = milling['Failure Label'].value_counts()

print("Tedad nemuneha baraye har barchasb:")
print(failure_counts)

# Nemudar mileyi
plt.figure(figsize=(8, 6))
sns.countplot(data=milling, x='Failure Label', hue='Failure Label', palette='viridis', legend=False)

plt.title('Tozie dastehaye 2ganeye Failure va No Failure', fontsize=14)
plt.xlabel('Barchasbe vaziate abzar', fontsize=12)
plt.ylabel('Tedade nemuneha', fontsize=12)
plt.show()

!pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
from collections import Counter

# Vizhegi
X = milling.drop(columns=['Failure Label', 'Failure Binary'])
# Hadaf
y = milling['Failure Label']

# Emale SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print("Tedad nemuneha pas az motevazen sazi:", Counter(y_resampled))

# Tedad nemuneha dar har daste
sns.countplot(x=y_resampled, hue=y_resampled, palette='viridis', legend=False)
plt.title('Tozie dastenhaye 2gane bad az motevazensazi')
plt.xlabel('Barchasbe vaziate abzar')
plt.ylabel('Tedad nemuneha')
plt.show()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
     X_resampled, y_resampled, test_size=0.2, random_state=42,
     stratify=y_resampled)

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

models = {'Logistic Regression': LogisticRegression(max_iter=1000),
          'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),
          'SVM (Linear Kernel)': SVC(kernel='linear'),
          'SVM (RBF Kernel)': SVC(kernel='rbf')}

for name, model in models.items():
    print(f"Model: {name}")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print(classification_report(y_test, y_pred))
    print("Matrix ashoftegi:")
    print(confusion_matrix(y_test, y_pred))

from sklearn.model_selection import cross_val_score

print("Natayeje Cross Validation:")
for name, model in models.items():
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    print(f"{name}: دقت میانگین = {scores.mean():.4f}, انحراف معیار = {scores.std():.4f}")

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

param_knn = {'n_neighbors': [3, 5, 7, 9, 11]}
grid_knn = GridSearchCV(KNeighborsClassifier(), param_knn, cv=5, scoring='accuracy')
grid_knn.fit(X_train, y_train)

print("Behtarin Parameterha baraye KNN:", grid_knn.best_params_)
print("Behtarin deghat:", grid_knn.best_score_)

from sklearn.linear_model import LogisticRegression

param_logreg = {'C': [0.01, 0.1, 1, 10],'penalty': ['l2'],
    'solver': ['lbfgs']}

grid_logreg = GridSearchCV(LogisticRegression(max_iter=1000), param_logreg, cv=5, scoring='accuracy')
grid_logreg.fit(X_train, y_train)

print("Behtarin Parameterha baraye Logistic Regression:", grid_logreg.best_params_)
print("Behtarin deghat:", grid_logreg.best_score_)

from sklearn.svm import SVC

param_svm_linear = {'C': [0.1, 1, 10],'kernel': ['linear']}

grid_svm_linear = GridSearchCV(SVC(), param_svm_linear, cv=5, scoring='accuracy')
grid_svm_linear.fit(X_train, y_train)

print("Behtarin Parameterha baraye SVM (Khati):", grid_svm_linear.best_params_)
print("Behtarin deghat:", grid_svm_linear.best_score_)

param_svm_rbf = {'C': [0.1, 1, 10],'gamma': [0.01, 0.1, 1],'kernel': ['rbf']}

grid_svm_rbf = GridSearchCV(SVC(), param_svm_rbf, cv=5, scoring='accuracy')
grid_svm_rbf.fit(X_train, y_train)

print("Behtarin Parameterha baraye SVM (Gheyre khati):", grid_svm_rbf.best_params_)
print("Behtarin deghat:", grid_svm_rbf.best_score_)

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier
from sklearn.metrics import classification_report, accuracy_score

X = milling.drop(columns=['Failure Label'])
y = milling['Failure Label']  # Barchasbe hadaf
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# KNN
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)
print("Deghate KNN:", accuracy_score(y_test, knn_pred))
print(classification_report(y_test, knn_pred))

# Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
dt_pred = dt.predict(X_test)
print("Deghate Decision Tree:", accuracy_score(y_test, dt_pred))
print(classification_report(y_test, dt_pred))

# Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)
print("Deghate Random Forest:", accuracy_score(y_test, rf_pred))
print(classification_report(y_test, rf_pred))

# SVM (One-vs-Rest)
svm = OneVsRestClassifier(SVC(kernel='linear', random_state=42))
svm.fit(X_train, y_train)
svm_pred = svm.predict(X_test)
print("Deghate SVM (One-vs-Rest):", accuracy_score(y_test, svm_pred))
print(classification_report(y_test, svm_pred))

# SVM (One-vs-One)
svm_ovo = OneVsOneClassifier(SVC(kernel='linear', random_state=42))
svm_ovo.fit(X_train, y_train)
svm_ovo_pred = svm_ovo.predict(X_test)
print("Deghate SVM (One-vs-One):", accuracy_score(y_test, svm_ovo_pred))
print(classification_report(y_test, svm_ovo_pred))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier
from collections import Counter

milling = pd.read_csv("milling_machine.csv")

# Barchasbgozarie chandgane
milling['Failure Types'] = milling['Failure Types'].fillna("No Failure")

# Pishpardazesh
numeric_features = milling.select_dtypes(include=['float64', 'int64']).columns
milling[numeric_features] = milling[numeric_features].fillna(milling[numeric_features].mean())

# Standardsazi
scaler = StandardScaler()
milling[numeric_features] = scaler.fit_transform(milling[numeric_features])

# Ramzgozarie barchasbha
le = LabelEncoder()
milling['Failure_Label_Multi'] = le.fit_transform(milling['Failure Types'])

# Namayeshe barchasbha
label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print("Mapping LabelEncoder:")
print(label_mapping)


X = milling.drop(columns=['Failure Types', 'Failure_Label_Multi'])
y = milling['Failure_Label_Multi']

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

sns.countplot(x=y_resampled)
plt.title("Tozi'e classha pas az SMOTE")
plt.xlabel("Class")
plt.ylabel("Tedad nemune")
plt.show()

X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42)

# Modelha
models = {
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM (OvR)': OneVsRestClassifier(SVC(kernel='rbf', random_state=42)),
    'SVM (OvO)': OneVsOneClassifier(SVC(kernel='rbf', random_state=42)),}
results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = (y_pred == y_test).mean()
    print(f"Model: {name}")
    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred, target_names=le.classes_))
    results[name] = acc

# KNN
param_grid_knn = {'n_neighbors': [3, 5, 7, 9]}
grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5, scoring='accuracy')
grid_knn.fit(X_train, y_train)

# Decision Tree
param_grid_tree = {'max_depth': [5, 10, 15],'min_samples_split': [2, 5, 10]}
grid_tree = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_tree, cv=5, scoring='accuracy')
grid_tree.fit(X_train, y_train)

# Random Forest
param_grid_rf = {'n_estimators': [50, 100],'max_depth': [5, 10, None]}
grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, scoring='accuracy')
grid_rf.fit(X_train, y_train)

# SVM OvR
param_grid_svm_ovr = {'estimator__C': [0.1, 1, 10],'estimator__gamma': [0.01, 0.1, 1]}
grid_svm_ovr = GridSearchCV(OneVsRestClassifier(SVC(kernel='rbf')), param_grid_svm_ovr, cv=5, scoring='accuracy')
grid_svm_ovr.fit(X_train, y_train)

# SVM OvO
grid_svm_ovo = GridSearchCV(OneVsOneClassifier(SVC(kernel='rbf')), param_grid_svm_ovr, cv=5, scoring='accuracy')
grid_svm_ovo.fit(X_train, y_train)

print("Behtarin parameterha:")
print("KNN:", grid_knn.best_params_)
print("Decision Tree:", grid_tree.best_params_)
print("Random Forest:", grid_rf.best_params_)
print("SVM (OvR):", grid_svm_ovr.best_params_)
print("SVM (OvO):", grid_svm_ovo.best_params_)

# from sklearn.model_selection import cross_val_score

# for name, model in models_multiclass.items():
#     scores = cross_val_score(model, X_multi_scaled, y_multi, cv=5)
#     print(f"{name}: میانگین دقت (5-Fold CV) = {scores.mean():.4f}")